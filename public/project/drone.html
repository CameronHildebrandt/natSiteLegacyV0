<!DOCTYPE html>
<html>

  <head>
    <title>NeurAlbertaTech</title>

    <!-- Website created by Cameron Hildebrandt -->

    <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon-16x16.png">
    <link rel="manifest" href="/images/favicon/site.webmanifest">
    <link rel="mask-icon" href="/images/favicon/safari-pinned-tab.svg" color="#7967e1">
    <meta name="apple-mobile-web-app-title" content="NeurAlbertaTech">
    <meta name="application-name" content="NeurAlbertaTech">
    <meta name="msapplication-TileColor" content="#603cba">
    <meta name="theme-color" content="#323232">

    <!-- Import JQuery -->
    <script type='text/javascript' src="https://code.jquery.com/jquery-1.11.2.js"></script>

    <!-- External CSS -->
    <link href="../style.css" rel="stylesheet" type="text/css">

    <!-- External JS -->
    <script type="text/javascript" src="../scrollBackground.js"></script>
    <script type="text/javascript" src="../burgerController.js"></script>

    <!-- Add Our Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Maven+Pro:wght@400;700;900&display=swap" rel="stylesheet">
    <meta name="viewport" content="width=device-width">
  </head>

  <body>

    <div id="burger">
      <div class="burgerPart" id="topBun"></div>
      <div class="burgerPart" id="patty"></div>
      <div class="burgerPart" id="bottomBun"></div>
    </div>

    <div class="mobileMenu">
      <a href="/index.html"><button class="mobilePageLink">Home</button></a>
      <a href="/projects.html"><button class="mobilePageLink">Projects</button></a>
      <a href="/team.html"><button class="mobilePageLink">Team</button></a>
      <a href="/events.html"><button class="mobilePageLink">Events</button></a>
      <a href="natflat.html"><button class="mobilePageLink">natFlat</button></a>
      <a href="community.html"><button class="mobilePageLink">Community</button></a>
      <a href="/contact.html"><button class="mobilePageLink">Contact</button></a>
    </div>

    <div id=headerBar>
      <a href="/index"><image id="headerImg" src="/images/nat.svg" alt="NAT Logo"></image></a>

      <a href="/contact.html"><button class="pageLink">Contact</button></a>
      <a href="community.html"><button class="pageLink">Community</button></a>
      <a href="natflat.html"><button class="pageLink">natFlat</button></a>
      <a href="/events.html"><button class="pageLink">Events</button></a>
      <a href="/team.html"><button class="pageLink">Team</button></a>
      <a href="/projects.html"><button class="pageLink">Projects</button></a>
      <a href="/index.html"><button class="pageLink">Home</button></a>
    </div>

    <div style="position: relative"><div class="spinner"></div></div>
    <img id="headerBGImg" lsrc="/images/ProjectPhotos/BrainDroneThumbnail.jpg"></img>
    <img id="headerBGDiv"></img>


    <div id=scrollContainer>
      <div id=divAboveMainForSnapping></div>
      <div id=divAboveMainForGradient></div>

      <div id="main">



        <h1 class="subTitle" style="margin-left: 2%;">NeurAlbertaTech Brain Drone</h1>
        <div id="largeInfoBlock">
          <div id="largeInfoContents" style="margin-top: 1%;">

            <div id="largeInfoBlockMedia">
              <div id="largeMediaFrame">
                <img src="../images/ProjectPhotos/BrainDroneThumbnail1.jpg" style="width: 100%;"></img>
              </div>
              <a href='https://github.com/neuralbertatech/openComp2020' target="_blank"><button id="smallButton">Github</button></a>
            </div>

            <div id="largeInfoBlockText">
              <div class="subSubTitle" style="margin-bottom: 15px;">NeuroTechX Open Competition 2020 Submission</div>
              <p class="paragraph">At the start of this year, we built a team and set out to build a drone controller for our OpenBCI headset. Thoughout the year, we decided to stick to our original plan of controlling a drone, but we realized that the controller that we were building was actually really generalizable. Once we made this realization, we made it a crucial factor of our code design and treated it as an overall goal for the finalized version of this project: build a BCI controller that can be hooked up to almost anything! </p>

            </div>
          </div>
        </div>




        <div id="largeInfoBlock">
          <div id="largeInfoContents" style="margin-top: 1%; display: block;">
            <div class="subTitle" style=" margin-bottom: 15px;">Functionality</div>

            <p class="paragraph">
              One of the fundamental objectives of this project was to make an application that runs as discretely as possible, allowing for a nearly seamless connection between the BCI and the drone once the initial setup is complete. The only time that you really need to interact with the computer directly is during the setup process. While flying the drone, the computer acts exclusively as a bridge and status display for the flgiht dashboard, as well as an emergency drone kill switch. You indeed control 100% of the drone's actions using the headset.
              <br><br>
              <b>1: Connect BCI</b><br>
              The very first thing that the program does is connect the OpenBCI to the computer. During this step, the program runs some diagnostic tests in the background to ensure that the BCI is connected properly.
              <br><br>
              <b>2: Collect Data</b><br>
              This portion of the program will either collect a baseline or load a previously collected baseline depending on how the program is run. When we start the baseline collection process, it will automatically begin displaying random commands for you to mimic. What exactly it means to mimic is defined by what queues you choose to use to fly the drone. The queues that we usually used when we flew the drone were a mixture of thinking of moving muscles, stimulating the brain by looking at different colours, and reading different words. Alternatively, for this step we can load the data collected in a previous session (assuming that the BCI hasn't been taken off since that data was recorded) to save a large portion of time when starting the program.
              <br><br>
              <b>3: Train Model</b><br>
              Training the models happens entirely automatically in the background. The program will take the recorded baseline brain data and pass it into three models for classification. All of the models that we used were simple LDA classifiers that trained on Fourier transformed data, averaged data (compressing all 16 channels into one "average" channel), and raw data (purely the signal from the headset). The program uses all three of these classifiers in conjunction to intelligently and confidently assign a single label to your brain data.
              <br><br>
              <img src="../images/ProjectPhotos/training.gif" id="exampleGif"></img>
              <br><br>
              <b>4: Conenct Drone</b><br>
              This step is fairly similar to the "Connect BCI" step. We simply connect the drone to the computer and finally establish the complete link between the BCI headset and the drone.
              <br><br>
              <b>5: Fly!</b><br>
              Once all of these preliminary steps have been completed, we are ready to fly the drone. At this point the only reason you would need to interact directly with the computer is to force the drone to preform an emergency landing. The program now automatically collects samples from the BCI and intelligently uses the 3 classifiers to predict what action you want the drone to take and then commands the drone to take that action. During this part of the program, the computer will also display a flight dashboard, which presents all of the crucial information like the drone's battery level, the prediction that each individual model is making, and the overall command that is being sent to the drone. See below an example of the dashboard.
              <br><br>
              <img src="../images/ProjectPhotos/flightDashboard.gif" id="exampleGif"></img>
            </p>
          </div>
        </div>




        <div id="largeInfoBlock">
          <div id="largeInfoContents" style="margin-top: 1%; display: block;">
            <div class="subTitle" style=" margin-bottom: 15px;">Data</div>

            <p class="paragraph">
              <b>What does the data look like?</b><br>
              The data is handled differently for each model that we have. Let's first walk through what we do to the data for the Fourier transform model.
              <br><br><br>

              <img src="../images/ProjectPhotos/dataExample/0.jpg" id="exampleGif" class="dataExample"></img>

              <br><br>
              <b>Fourier Model</b>
              <p id="dataParagraph" class="paragraph">
                This is what the raw data from the OpenBCI looks like. Each graph represents a one second window of one of the 16 channels, hence why there is 16 graphs for the single sample. You may notice that this data is actually quite spiky and sporradic. While it is true that some of that spiky data represents high frequency (gamma) brain waves, the grand majority of it is actually noise and imperfect representations of our data. A rather straightforward thing that we can do to muffle the noise and "hone in" on the brain signal is just smooth out the curve. We can apply something called a Savitzky-Golay (savgol) filter, which will do precicely that. You can read more about how exactly a savgol filter works <a href="https://en.wikipedia.org/wiki/Savitzky-Golay_filter" id="uncoloredLink" target="_blank">here</a>. <br><br><b>Click on the data to apply the savgol filter.</b>
              </p>

              <p class="paragraph">
                <br><br><br>
                <b>Average Model</b><br>
                This model is drastically simpler than the Fourier model. We still pass all of the data through the savgol filter to have smoother samples, but this time, we simply add all of the samples together to create a single "sample" that is really good at capturing spikes that occur over all channels. We then train the model directly off of this.

                <br><br><br>
                <b>Raw Model</b><br>
                This model is definitely the simplest of our models. We once again pass the data through a savgol filter and then concatenate all of the samples into one long sample. We then leave 100% of the interpretation of the data up to the model. Depending on the training data, this can lead to surprisingly strong predicting power, sometimes as much as 20% stronger than the next strongest model.
              </p>
            </p>
          </div>
        </div>



        <div id="largeInfoBlock">
          <div id="largeInfoContents" style="margin-top: 1%; display: block;">
            <div class="subTitle" style=" margin-bottom: 15px;">Future Potential</div>
            <p class="paragraph">
              <b>What Can we do With the Controller?</b><br>
              As briefly mentioned above, from the very beginning of this project, we designed our code base to be expandable. We realized that the simple act of classifying directional labels from brain data has a number of applications beyond just flying a drone. A number of incredibly powerful and potentially life changing applications. Realistically, anyone (since our code base is open source) can take our classifier and attach it to any other peripheral by modifying just a few lines of code. The future possibilities of what exactly this can be connected to is essentially limitless. We chose to connect our controller to a drone to show it's power in an entertaining and fun way, though, this can really be connected to anything from games to wheelchairs to rudamentary speech tools to allow for a new method of control that enables people to meaningfully interact with the world, regardless of motor ability.
              <br><br>
              <b>What Can we do to Improve the Prediction Quality in the Future?</b><br>
              The most obvious way to improve our prediction quality in the future over the long term is to take a much more data-driven approach to the problem. One of the largest drawbacks of our current implementation is that the baseline that is collected must constitue of 100% of the data that will be used for training. This means that we are immediately faced with a compromise, we have to pick a reasonably small amount of time to collect the baseline, so that the setup for flying the drone does not take all day, but we also need to pick a reasonably large amount of time to collect as many baseline samples as possible so that the quality of our model is high enough that it is actually useful.
              <br><br>
              The way that we can circumvent needing to make this tradeoff in the first place is by collecting and saving data. A lot of data. Then we can train a model on our large dataset to get reasonably strong predicting power. We can then also use the baseline collection time to collect data that will be used to tailor the existing model to the user, as opposed to creating an entirely new model from scratch, meaning that the time needed to collect the baseline will also be drastically reduced.
              <br><br>
              This data-driven approach is actually the first plan that we had for this program. The biggest problem that we faced with the approach, though, is that the sheer magnitude of data necessary for it to outpreform our "run-n-gun" approach is massive. When the headset is taken off and put back on between sessions, the exact position of the electrodes slightly changes, which entirely invalidates the predicting power of the little nuances that the "run-n-gun" aproach thrives off of. To counter this, it is necessary to collect a massive amount of data.
              <br><br>
              This need for big data did not scare us, though. We spend months collecting nearly 6000 data samples to begin training this model. By the time that we actually had a prototype for the model up and running, however, we made the unfortunate discovery that there was a one-line error in the training data collection script that invalidated approximately 95% of our dataset. The worst part was that there was also really no way for us to know which of these samples were actually valid. We had to completely pivot the approach that we were taking to solve the problem.
              <br><br>
              Fortunately, within a short time constraint, the team was able to very efficently put together the "run-n-gun" model, which ended up preforming much better than we first anticipated. In conclusion, we firmly believe that we can greatly improve the overall quality of our classifier in the long term by switching over to the data-driven approach.
            </p>
          </div>
        </div>



        <div id="largeInfoBlock">
          <div id="largeInfoContents" style="display: block;">
            <div class="subTitle" style="text-align: center; margin-bottom: 1%;">Submission Video: <span style="font-weight: 300; font-size: 40px;">The Brain Controlled Drone</span></div>

            <div id="largeMediaFrame" style="height: 516px; width: 917px; margin: 0 auto;">

              <!-- <iframe id="smallVideoPlayer" frameborder="0" scrolling="no" marginheight="0" marginwidth="0" width="300.375" height="168.75" type="text/html" src="https://www.youtube.com/embed/32io4iy6L0k?autoplay=0&fs=1&iv_load_policy=3&showinfo=1&rel=0&cc_load_policy=0&start=0&end=0&origin=https://youtubeembedcode.com">
              </iframe> -->

              <div style="overflow:hidden;position: relative;" id="smallVideoPlayer">
                <iframe frameborder="0" scrolling="no" marginheight="0" marginwidth="0"width="300.375" height="168.75" type="text/html" src="https://www.youtube.com/embed/-3UTwKpKpcI?autoplay=0&fs=1&iv_load_policy=3&showinfo=1&rel=0&cc_load_policy=0&start=0&end=0&vq=hd1080">
                </iframe>

                <div style="position: absolute;bottom: 10px;left: 0;right: 0;margin-left: auto;margin-right: auto;color: #000;text-align: center;">
                  <small style="line-height: 1.8;font-size: 0px;background: #fff;">
                    <a href="https://www.altcodesymbols.com/" rel="nofollow">Special characters</a>
                  </small>
                </div>

                <style>.newst{position:relative;text-align:right;height:420px;width:520px;} #gmap_canvas img{max-width:none!important;background:none!important}</style>
              </div>

              <iframe id="normalVideoPlayer" width="917" height="516" src="https://www.youtube.com/embed/-3UTwKpKpcI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

            </div>
          </div>
        </div>



        <!-- We don't actually have mmedia coverage yet, so let's just hide the whole block -->
        <!-- <div id="largeInfoBlock">
          <div id="largeInfoContents" style="margin-top: 1%; display: block;">
            <p class="paragraph" style="margin-top: 20px; text-align: center;"><span style="font-weight: 700;">Check out our media coverage:</span></p>
            <div id="mediaGrid">

              <div class="mediaLogo">
                <a href="https://www.cbc.ca/news/canada/edmonton/u-of-a-students-create-video-game-uses-brain-controller-1.5263352" target="_blank">
                  <image src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/CBC_Logo_1992-Present.svg/1024px-CBC_Logo_1992-Present.svg.png"></image>
                </a>
              </div>

              <div class="mediaLogo">
                <a href="https://edmonton.ctvnews.ca/brain-controlled-video-game-aims-to-improve-accessibility-for-players-1.4567521?cache=yes%3FclipId%3D89531%3FautoPlay%3Dtrue%3FautoPlay%3Dtrue%3FautoPlay%3Dtrue%3Fot%3DAjaxLayout%3FautoPlay%3Dtrue%3FautoPlay%3Dtrue%3FautoPlay%3Dtrue%3Fot%3DAjaxLayout%3Fot%3DAjaxLayout%3FautoPlay%3Dtrue%3Fot%3DAjaxLayout%3FautoPlay%3Dtrue%3FautoPlay%3Dtrue" target="_blank">
                  <image src="https://upload.wikimedia.org/wikipedia/commons/5/5e/CTV_flat_logo.svg"></image>
                </a>
              </div>

              <div class="mediaLogo">
                <a href="https://edmontonjournal.com/news/local-news/u-of-a-students-create-mind-controlled-video-game/" target="_blank">
                  <image src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Edmonton_Journal_%282020-01-15%29.svg/1200px-Edmonton_Journal_%282020-01-15%29.svg.png"></image>
                </a>
              </div>


              <div class="mediaLogo">
                <a href="https://www.thestar.com/calgary/2019/08/29/some-alberta-university-students-have-created-a-mind-controlled-video-game-and-the-technology-could-have-real-world-applications.html" target="_blank">
                  <image src="https://lh3.googleusercontent.com/rNE-n57GBBDjcxbLwY2ouar-aM_ufghjPmpFp5xyEMyhRPxblYXZ0Nx12Z79LIYniGySiB4fo8hr96bJGwqa8kVqT36qLC6m_8pijVx-M6VnoP3oj7xVytWtS6nBfznnh0tZQiaWxQ=w2400"></image>
                </a>
              </div>


              <div class="mediaLogo">
                <a href="https://www.facebook.com/CityNewsYEG/videos/2097476683892344/" target="_blank">
                  <image src="https://lh3.googleusercontent.com/CHVewaOa751dAQnzsPp7KLIaBkRvkuorfJaw0oo7S8X1fqMkDyefiJ2-rwQ3Ux6np5gKde7hvB3QjOhoCcxWOJXx-urQAiSUlhyAljiWcKbZyh0bt-MoxShPaR6ZJlTXV5mFwVfqOA=w2400"></image>
                </a>
              </div>

              <div class="mediaLogo">
                <a href="https://novo.press/university-students-create-first-mind-controlled-video-game-using-neurotechnology/" target="_blank">
                  <image src="https://novo.press/wp-content/uploads/2017/08/NOVO-logo-1.png"></image></a>
                </a>
              </div>

              <div class="mediaLogo">
                <a href="https://www.ualberta.ca/science/news/2019/august/neurotechnology-video-game.html" target="_blank">
                  <image src="../images/HostedOnGooglePhotos/MediaLogos/UAlberta.png"></image></a>
                </a>
              </div>

            </div>
          </div>
        </div> -->

        <div id="interestedBlock" style="margin-bottom: 30px;">

          <div id="interestedBlockContents">

          <span style="color: rgba(255,255,255,1); font-size: 35px; font-weight: 900; font-family: 'Maven Pro', sans-serif;">2020 Project Team</span>
          <!-- <div id="smallImage" style="width: 100%; margin-top: 10px; margin-bottom: 30px; height: 100%;">
            <img src="../images/Headshots/CameronHeadshot2.jpg" style="width: 100%"></img>
          </div> -->


          <!-- <div style="color: rgba(255,255,255,1); font-size: 35px; font-weight: 900; font-family: 'Maven Pro', sans-serif;">Members</div> -->

          <div id="viewPastExecsContentScrollContainer">
            <table style="width:100%" class="pastExecsTable">
              <tr>
                <th>
                  <div id="smallImage">
                    <a href="https://www.linkedin.com/in/cdhildebrandt/" target="_blank"><img src="../images/Headshots/CameronHeadshot2.jpg" id="smallImageChild"></img></a>
                  </div>
                </th>

                <th>
                  <div id="smallImage">
                    <a href="https://www.linkedin.com/in/zachary-selk-263b541b4/" target="_blank"><img src="../images/Headshots/ZachHeadshot.jpg" id="smallImageChild"></img></a>
                  </div>
                </th>

                <th>
                  <div id="smallImage">
                    <a href="https://www.linkedin.com/in/landon-fuhr-8522031b7/" target="_blank"><img src="../images/Headshots/LandonHeadshot.jpg" id="smallImageChild"></img></a>
                  </div>
                </th>

                <th>
                  <div id="smallImage">
                    <a href="https://www.linkedin.com/in/ryan-y-k/" target="_blank"><img src="../images/ProjectPhotos/brainDroneTeam/ryan.jpg" id="smallImageChild"></img></a>
                  </div>
                </th>

                <th>
                  <div id="smallImage">
                    <a href="https://www.linkedin.com/in/nevil-kandathil/" target="_blank"><img src="../images/ProjectPhotos/brainDroneTeam/nevil.jpg" id="smallImageChild"></img></a>
                  </div>
                </th>

                <th>
                  <div id="smallImage">
                    <a href="https://www.linkedin.com/in/eden-redman-neuro/" target="_blank"><img src="../images/Headshots/EdenHeadshot.jpg" id="smallImageChild"></img></a>
                  </div>
                </th>

                <th>
                  <div id="smallImage">
                    <a href="https://www.linkedin.com/in/david-lam-97866115b/" target="_blank"><img src="../images/Headshots/DavidHeadshot.jpg" id="smallImageChild"></img></a>
                  </div>
                </th>

                <th>
                  <div id="smallImage">
                    <a href="https://www.linkedin.com/in/matthew-danyluik/" target="_blank"><img src="../images/Headshots/MatthewHeadshot.jpg" id="smallImageChild"></img></a>
                  </div>
                </th>

                <th>
                  <div id="smallImage">
                    <img src="../images/Headshots/JiyeonHeadshot.jpg" id="smallImageChild"></img>
                  </div>
                </th>

                <!-- <th>
                  <div id="smallImage">
                    <img src="../images/InfoNight/Error.jpg" id="smallImageChild"></img>
                  </div>
                </th> -->
              </tr>


              <tr style="font-family: 'Maven Pro'; color: rgba(255,255,255,1);">
                <th>Cameron Hildebrandt</th>
                <th>Zach Selk</th>
                <th>Landon Fuhr</th>
                <th>Ryan Kang</th>
                <th>Nevil Kadanthil</th>
                <th>Eden Redman</th>
                <!-- <th>Harrison Fah</th>
                <th>Adam Elamy</th>
                <th>Marty Rudolf</th> -->
                <th>David Lam</th>
                <th>Matthew Danyluk</th>
                <th>Jiyeon Seo</th>
                <!-- <th>Eddie Guo</th>
                <th>Anna Romero</th>
                <th>Carina Zhou</th> -->


              </tr>

              <tr style="font-family: 'Maven Pro'; color: rgba(255,255,255,1);">
                <td>Team Lead</td>
                <td>Administrative / Software</td>
                <td>Software</td>
                <td>Software</td>
                <td>Software</td>
                <td>Software</td>
                <td>Research</td>
                <td>Research</td>
                <td>Research</td>
              </tr>
            </table>
          </div>

        </div>

      </div>
    </div>


    <script type="text/javascript">
      // To allow Burger to Work
      menuToggle();
    </script>

    <script type="text/javascript">

      $(document).ready(function(){
        $('#viewPastExecsButton').click(function(){
          if($('#viewPastExecsButton').text() === "View 2019 Project Team") {

            // Button
            $('#viewPastExecsButton').text("Back");
            $('#viewPastExecsButton').css('background-color', 'rgba(0, 0, 0, 1)');

            // Content
            $('#viewPastExecsContent').css("height","500px"); // so we never hit it
            $('#viewPastExecsContent').css("margin-top","-50px");
            $('#viewPastExecsContent').css("opacity","100");
            $('#viewPastExecsContent').css("padding-bottom","2%");
            $('#viewPastExecsContent').css("padding-top","70px");
            // $('#viewPastExecsContent').css("transform","scaleY(1)");

            // Container
            $('#viewPastExecsContentContainer').css("display","block");
          } else {
            // Button
            $('#viewPastExecsButton').text("View 2019 Project Team");
            $('#viewPastExecsButton').css("background-color", "rgba(0, 0, 0, 0.8)");

            // Content
            $('#viewPastExecsContent').css("height","0px");
            $('#viewPastExecsContent').css("margin-top","0px");
            $('#viewPastExecsContent').css("opacity","0");
            $('#viewPastExecsContent').css("padding-bottom","0px");
            $('#viewPastExecsContent').css("padding-top","0px");
            // $('#viewPastExecsContent').css("transform","scaleY(0)");

            // Container
            $('#viewPastExecsContentContainer').css("display","none");
          }
        });
      });

    </script>


    <!-- Show data processing -->
    <script>
      $(document).ready(function(){
        var currStage = 0;
        var dataParagraph = document.getElementById("dataParagraph");
        var stageParagraphs = [
          'This is what the raw data from the OpenBCI looks like. Each graph represents a one second window of one of the 16 channels, hence why there is 16 graphs for the single sample. You may notice that this data is actually quite spiky and sporradic. While it is true that some of that spiky data represents high frequency (gamma) brain waves, the grand majority of it is actually noise and imperfect representations of our data. A rather straightforward thing that we can do to muffle the noise and "hone in" on the brain signal is just smooth out the curve. We can apply something called a Savitzky-Golay (savgol) filter, which will do precicely that. You can read more about how exactly a savgol filter works <a href="https://en.wikipedia.org/wiki/Savitzky-Golay_filter"  id="uncoloredLink" target="_blank">here</a>. <br><br><b>Click on the data to apply the savgol filter.</b>',

          "Now that we have smoothed out the data, it is much easier to see the underlying waves, which means that the Fourier transform will likely be able to better represent what is actually going on with the data. Let's pass each of the individual channels through a Fourier transform. <br><br><b>Click on the data to pass the channels through a Fourier transform.</b>",

          "Fantastic! Though it may not look like it, you are looking at the exact same data. The Fourier transform doesn't actually change the data at all, it simply changes the way we represent it. Here the data is represented in the frequency domain instead of the time domain, so the different peaks of these graphs show relative strengths of each frequency in the overall signal as opposed to showing the overal signal itself. It may not be immediately obvious what's going on here, but this is actually really powerful. This transform allows us to use our neuroscience knowledge to further interpret this data. <br><br><b>Click on the data to further analyze the data.</b>",

          "This is the exact same thing, but now we have fancy lines drawn on it! These lines are placed to break up the data into the regions of 0.5-4 Hz, 4-7.5 Hz, 7.5-12 Hz, 12-30 Hz, and 30-100 Hz. These regions correspond to the delta, theta, alpha, beta, and gamma power bands respectively. What we want to do now is calculate a numerical representation of the relative strength of each of these bins. To do that, we simply average every point in each of these bins. <br><br><b>Click on the data to find the binned values.</b>",

          "Now we have an incredibly simple representation of the entire sample that contains just 5 numbers, the relative strength of each of the power bins, for each of the 16 channels. Having these bins of data is incredibly powerful, as we can interpret the data over time to see how it changes, and we can even compare how these bins change over time compared to each other to determine rather complex occurances in the brain, such as mood, attention level, and calmness. For our classifier application, however, we sort of skim over this nuance. We don't exactly know how your calmness, attention, etc. correlate to our directional labels, so we are just going to directly feed this data into the LDA classifier and leave it to discover that for us. Let's compress this data a little more. <br><br><b>Click on the data to compress this data further.</b>",

          "Here we have simply averaged the strength of all of the bins to discover 5 numbers that represent the entire sample. We have now preprocessed one sample for the Fourier model! The program follows this process for every sample that we record to build a dataset of these bits of data above that will all go towards training our Fourier LDA Model.<br><br><b>Click on the data to reset this exercise.</b>"];


        // Preload images to make activity super snappy
        var i;
        for (i = 0; i <= 5; i++) {
          var img=new Image();
          img.src="../images/ProjectPhotos/dataExample/" + i + '.jpg';
        }

        $('.dataExample').click(function(){
          if(currStage < 5){
            currStage = currStage + 1
          }
          else {
            currStage = 0
          }

          $('.dataExample').attr('src', '../images/ProjectPhotos/dataExample/' + currStage + '.jpg');
          dataParagraph.innerHTML = stageParagraphs[currStage];
        });
      })
    </script>

    <!-- Dynamically Load Header -->
    <script type="text/javascript" src="/dynamicLoad.js"></script>

  </body>

</html>
